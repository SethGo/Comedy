{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install seaborn\n",
    "!pip3 install sklearn\n",
    "!pip3 install keras\n",
    "!pip3 install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import seaborn as sns; sns.set(color_codes=True)\n",
    "import re \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>tags</th>\n",
       "      <th>date_posted</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>year</th>\n",
       "      <th>transcript</th>\n",
       "      <th>language</th>\n",
       "      <th>runtime</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chris D’Elia: Man on Fire</td>\n",
       "      <td>Chris D'Elia,Stand-up transcripts</td>\n",
       "      <td>April 19th, 2020</td>\n",
       "      <td>https://scrapsfromtheloft.com/2020/04/19/chris...</td>\n",
       "      <td>Chris D'Elia</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>Fire Fire  Man on fire Fire Fire Fire Man on ...</td>\n",
       "      <td>en</td>\n",
       "      <td>65.0</td>\n",
       "      <td>6.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>George Carlin: The Indian Drill Sergeant</td>\n",
       "      <td>George Carlin,Stand-up transcripts</td>\n",
       "      <td>April 13th, 2020</td>\n",
       "      <td>https://scrapsfromtheloft.com/2020/04/13/georg...</td>\n",
       "      <td>George Carlin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In The Indian Sergeant, was emerging as George...</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tom Segura: Ball Hog</td>\n",
       "      <td>Stand-up transcripts,Tom Segura</td>\n",
       "      <td>March 25th, 2020</td>\n",
       "      <td>https://scrapsfromtheloft.com/2020/03/25/tom-s...</td>\n",
       "      <td>Tom Segura</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>How you doing? No shit. Thank you, thank you...</td>\n",
       "      <td>en</td>\n",
       "      <td>70.0</td>\n",
       "      <td>7.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bert Kreischer: Hey Big Boy</td>\n",
       "      <td>Bert Kreischer,Stand-up transcripts</td>\n",
       "      <td>March 21st, 2020</td>\n",
       "      <td>https://scrapsfromtheloft.com/2020/03/21/bert-...</td>\n",
       "      <td>Bert Kreischer</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>Ladies and gentlemen Bert Kreischer! Yeah! Wh...</td>\n",
       "      <td>en</td>\n",
       "      <td>62.0</td>\n",
       "      <td>7.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bert Kreischer: Fighting a Bear</td>\n",
       "      <td>Bert Kreischer,Stand-up transcripts</td>\n",
       "      <td>March 21st, 2020</td>\n",
       "      <td>https://scrapsfromtheloft.com/2020/03/21/bert-...</td>\n",
       "      <td>Bert Kreischer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The machine, Mr. Bert Kreischer, everybody. Le...</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        title  \\\n",
       "0               Chris D’Elia: Man on Fire       \n",
       "1  George Carlin: The Indian Drill Sergeant     \n",
       "2                    Tom Segura: Ball Hog       \n",
       "3             Bert Kreischer: Hey Big Boy       \n",
       "4            Bert Kreischer: Fighting a Bear    \n",
       "\n",
       "                                  tags       date_posted  \\\n",
       "0    Chris D'Elia,Stand-up transcripts  April 19th, 2020   \n",
       "1   George Carlin,Stand-up transcripts  April 13th, 2020   \n",
       "2      Stand-up transcripts,Tom Segura  March 25th, 2020   \n",
       "3  Bert Kreischer,Stand-up transcripts  March 21st, 2020   \n",
       "4  Bert Kreischer,Stand-up transcripts  March 21st, 2020   \n",
       "\n",
       "                                                link            name    year  \\\n",
       "0  https://scrapsfromtheloft.com/2020/04/19/chris...    Chris D'Elia  2017.0   \n",
       "1  https://scrapsfromtheloft.com/2020/04/13/georg...   George Carlin     NaN   \n",
       "2  https://scrapsfromtheloft.com/2020/03/25/tom-s...      Tom Segura  2020.0   \n",
       "3  https://scrapsfromtheloft.com/2020/03/21/bert-...  Bert Kreischer  2020.0   \n",
       "4  https://scrapsfromtheloft.com/2020/03/21/bert-...  Bert Kreischer     NaN   \n",
       "\n",
       "                                          transcript language  runtime  rating  \n",
       "0   Fire Fire  Man on fire Fire Fire Fire Man on ...       en     65.0     6.6  \n",
       "1  In The Indian Sergeant, was emerging as George...       en      NaN     NaN  \n",
       "2    How you doing? No shit. Thank you, thank you...       en     70.0     7.3  \n",
       "3   Ladies and gentlemen Bert Kreischer! Yeah! Wh...       en     62.0     7.1  \n",
       "4  The machine, Mr. Bert Kreischer, everybody. Le...       en      NaN     NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data from csv\n",
    "df = pd.read_csv('StandUpData.csv')\n",
    "df.loc[df.language == 'en']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With extra whitespace: \n",
      " Fire Fire  Man on fire Fire Fire Fire Man on fire Yes. Yes. All right. Yeah. All right. Just relax.\n",
      "Without extra whitespace:\n",
      "Fire Fire Man on fire Fire Fire Fire Man on fire Yes. Yes. All right. Yeah. All right. Just relax. W\n"
     ]
    }
   ],
   "source": [
    "# Remove leading, trailing, and intermitent whitespace\n",
    "print(\"With extra whitespace: \")\n",
    "print(df.iloc[0].transcript[:100])\n",
    "df.transcript = df.transcript.apply(lambda x: re.sub(\"\\s+\", \" \", x).strip())\n",
    "print(\"Without extra whitespace:\")\n",
    "print(df.iloc[0].transcript[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n",
      "['F', 'i', 'r', 'e', ' ', 'M', 'a', 'n', 'o', 'f', 'Y', 's', '.', 'A', 'l', 'g', 'h', 't', 'J', 'u', 'x', 'W', 'c', 'k', ',', 'd', 'C', 'I', 'm', 'p', 'y', 'b', 'L', 'w', 'U', '?', 'z', 'H', 'E', 'T', ':', 'j', 'v', 'N', 'R', 'S', 'O', 'B', '-', 'P', 'q', 'D', '!', 'V', 'G', 'K', '–', '—', ';', '/', 'Z', \"'\", 'é', 'ú', '%', '*', 'Q', '$', 'X', '#', '″', '£', '&', 'ç', 'í', 'ö', 'ä', 'ô', 'ñ', 'Ç', 'è', '@', 'ü', 'Ü', 'ó', '¿', 'á', 'É', 'Ó', '×', '\\\\', ']', '¡', '_', 'å', 'à', '®', 'ê', 'ù', 'Ã', 'ƒ', 'Â', 'º', '`', '+', 'Á', 'ì', 'È', 'ò', '′', '¶', 'û', '♫', 'Ö', '¢', 'ﬂ', '♬', '»', '=', '\\u200b', 'â', '~', '>']\n"
     ]
    }
   ],
   "source": [
    "# Collect all charcters to see which ones are present in the transcripts\n",
    "chars = []\n",
    "df.transcript.apply(lambda x: [chars.append(char) for char in x if char not in chars])\n",
    "\n",
    "print(len(chars))\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all unwanted characters\n",
    "remove = ['-', '–', '—', '/', 'é', 'ú', '%', '*', '$', '#', '″', '£', '&', 'ç', 'í', 'ö', 'ä', 'ô', 'ñ', 'Ç', 'è', '@', 'ü', 'Ü', 'ó', '¿', 'á', 'É', 'Ó', '×', '\\\\', ']', '¡', '_', 'å', 'à', '®', 'ê', 'ù', 'Ã', 'ƒ', 'Â', 'º', '`', '+', 'Á', 'ì', 'È', 'ò', '′', '¶', 'û', '♫', 'Ö', '¢', 'ﬂ', '♬', '»', '=', '\\u200b', 'â', '~', '>']\n",
    "df.transcript = df.transcript.apply(lambda x: ''.join([char for char in x if char not in remove]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13166511"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ''\n",
    "for i in df.transcript:\n",
    "    text += (i + ' ')\n",
    "\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1316651"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shorten text by 1/10 to save processing time\n",
    "portion = int(len(text)/10)\n",
    "text = text[:portion]\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 60\n",
      "nb sequences: 438871\n",
      "Vectorization...\n",
      "Build model...\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 40, 256)           324608    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 40, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 40, 256)           525312    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 40, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 60)                15420     \n",
      "=================================================================\n",
      "Total params: 1,390,652\n",
      "Trainable params: 1,390,652\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "438871/438871 [==============================] - 373s 849us/step - loss: 2.1005\n",
      "\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"rk, which is the publics ongoing inabili\"\n",
      "rk, which is the publics ongoing inabiling the thing the word and I was like, I dont know what the thing the word of the best the stare the way the sere the word the because you got a women \n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"rk, which is the publics ongoing inabili\"\n",
      "rk, which is the publics ongoing inabiling. So a bats the women. I was a bind, you know what this a chow her on the the beand to get of your of it. Okay? If you up a women, the part of the f\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"rk, which is the publics ongoing inabili\"\n",
      "rk, which is the publics ongoing inabilige fuves. Hor, you know Im like, Ceres dont cereuve is. Mrasabi come is Slipplinl. Helr I dont Maspy, stalf it, like, aw youge and our exores. Okayul,\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"rk, which is the publics ongoing inabili\"\n",
      "rk, which is the publics ongoing inabilitis night, Gow his out. LAUGHTER Tomawh, crick in over? Its so go Deople helld come hors me Claig on I fud my leokthe owher ong, Kead cend und Car! No\n",
      "\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.10049, saving model to model_best.h5\n",
      "Epoch 2/60\n",
      "438871/438871 [==============================] - 372s 847us/step - loss: 1.6098\n",
      "\n",
      "----- Generating text after Epoch: 1\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"s that make me one change a lifestyle ju\"\n",
      "s that make me one change a lifestyle just a lot of the back and the world and I was like, What was in the back and then you see the show and then you want to have a come of the could be a c\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"s that make me one change a lifestyle ju\"\n",
      "s that make me one change a lifestyle just got a couch your guy on the pool and I was like, Oh, I was gonna go to who do it was that the path of it would have here more in the joke in the ga\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"s that make me one change a lifestyle ju\"\n",
      "s that make me one change a lifestyle jubting for stop. But its not a fincing youre metter amery that ginams room up you have to have to sue it has goes behornedif that boin, fuck harrigat l\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"s that make me one change a lifestyle ju\"\n",
      "s that make me one change a lifestyle just lakm. I said it isIt it? HE be a wreat . Fuck around off a sunts, diverto com abcond as Mkdin. Then you think thatd a little at you Twen it allew k\n",
      "\n",
      "\n",
      "Epoch 00002: loss improved from 2.10049 to 1.60976, saving model to model_best.h5\n",
      "Epoch 3/60\n",
      "438871/438871 [==============================] - 373s 849us/step - loss: 1.4975\n",
      "\n",
      "----- Generating text after Epoch: 2\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"Dont you talk like the president. I done\"\n",
      "Dont you talk like the president. I done want to be a show the fucking shows and she was like, No, I was like, You know? I was like, You know, and I was like, You know, you know? I dont know\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"Dont you talk like the president. I done\"\n",
      "Dont you talk like the president. I done believe the thing they were in a creating saying on the sext of in the shiters and you know what I got the world, then the parent was so more the res\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"Dont you talk like the president. I done\"\n",
      "Dont you talk like the president. I done and then I wast totally your morning living, we dont make sthe just have for brevent. Pook. Nosting me a chance egorctian on a super Show stop crass,\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"Dont you talk like the president. I done\"\n",
      "Dont you talk like the president. I done? Wakan, one., because when maror, he goes, This is that with you. If as it allewcet a Fer, poD getting. And Im not a graid lobce it. If youre a whish\n",
      "\n",
      "\n",
      "Epoch 00003: loss improved from 1.60976 to 1.49752, saving model to model_best.h5\n",
      "Epoch 4/60\n",
      "438871/438871 [==============================] - 372s 847us/step - loss: 1.4341\n",
      "\n",
      "----- Generating text after Epoch: 3\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"y would you moan so loud? You know how d\"\n",
      "y would you moan so loud? You know how do you do that back off the world is a shop the first thing that should be like, We dont know what I was like, You know, I was like, What do you do tha\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"y would you moan so loud? You know how d\"\n",
      "y would you moan so loud? You know how do you think we can look at a guy that was some because I was like, No, I think the fuck you dont wanna be fucking fucking more pictures in the things \n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"y would you moan so loud? You know how d\"\n",
      "y would you moan so loud? You know how did anyone doesnt like a tamdes. I put you. Yeah, I was in the coiret of youk fucking care. Uh, Im a doctor now, clonger sleecing up for too matting fo\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"y would you moan so loud? You know how d\"\n",
      "y would you moan so loud? You know how do youg sDime so murder proopr. Its not a lean too. Whens thats hairoands, you is a placs. Anjoban. LAUGHR,TNNbLation dung off? This is not in misclefo\n",
      "\n",
      "\n",
      "Epoch 00004: loss improved from 1.49752 to 1.43415, saving model to model_best.h5\n",
      "Epoch 5/60\n",
      "438871/438871 [==============================] - 373s 850us/step - loss: 1.3900\n",
      "\n",
      "----- Generating text after Epoch: 4\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"anyways, Im a married man with a kid. Ha\"\n",
      "anyways, Im a married man with a kid. Have you see a bit of the real thing and the woman and then I was like, I dont know what I was like, What do you say that we have a good thing that I wa\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"anyways, Im a married man with a kid. Ha\"\n",
      "anyways, Im a married man with a kid. Have you can be like, I dont know whats happing them and I look at the show and then it was him that more and be a socceetion. They were like, Its a day\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"anyways, Im a married man with a kid. Ha\"\n",
      "anyways, Im a married man with a kid. Have another from is a sort of TVupels, but, to be shed smal milly, right? I know why were gone to hower! Pucked to me. Moarly caffers billion I ran abo\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"anyways, Im a married man with a kid. Ha\"\n",
      "anyways, Im a married man with a kid. Have your games. Im gone up as hit, if Ive hadget and enverone from sure Im encimers. I could Vomby, In blacket, Im sidery. I mele, this picse behind he\n",
      "\n",
      "\n",
      "Epoch 00005: loss improved from 1.43415 to 1.39001, saving model to model_best.h5\n",
      "Epoch 6/60\n",
      "438871/438871 [==============================] - 373s 849us/step - loss: 1.3571\n",
      "\n",
      "----- Generating text after Epoch: 5\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" story. Ill tell you a fun story, all ri\"\n",
      " story. Ill tell you a fun story, all right? I was like, What happened to the part of the world of a shite with the back of the condoms of the world that was what I was like, What do you get\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" story. Ill tell you a fun story, all ri\"\n",
      " story. Ill tell you a fun story, all right? You know what? They dont know what Im trying to be the pornon. And then Im like, Well, the shell his bath of the show was a lot of the months. Th\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" story. Ill tell you a fun story, all ri\"\n",
      " story. Ill tell you a fun story, all right? Youll be the rice. LAUGHTER  You dont challed some of Bow Youich the plome beer. Who just watchead the laffrour side with bilth of hero, and I sa\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" story. Ill tell you a fun story, all ri\"\n",
      " story. Ill tell you a fun story, all right? Hurt Daila? What maybe you ran rycome. Then falm, yalls on. and Then were giens and Kicable in Thery Marr out God! Hungs . It would stometal caus\n",
      "\n",
      "\n",
      "Epoch 00006: loss improved from 1.39001 to 1.35714, saving model to model_best.h5\n",
      "Epoch 7/60\n",
      "438871/438871 [==============================] - 372s 847us/step - loss: 1.3301\n",
      "\n",
      "----- Generating text after Epoch: 6\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"t more fun to do history that way. Like,\"\n",
      "t more fun to do history that way. Like, I dont know what I was like, What I dont know what I did anything that I dont know how much are you what I dont know what I was like, I dont know wha\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"t more fun to do history that way. Like,\"\n",
      "t more fun to do history that way. Like, I dont know what I said to were all the worst because I dont know what happened to people who do anything about his mom. That talked about the end of\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"t more fun to do history that way. Like,\"\n",
      "t more fun to do history that way. Like, why are you internet to her, each oitle. Ive had to have a nitter than, you used to shobe my phone quidty off the under but and fatter of Daven? CHUK\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"t more fun to do history that way. Like,\"\n",
      "t more fun to do history that way. Like, I cemes like, Traiat, the regreck all their own humber. Thats usef backlating in Get, but I Amey, The sorbri, you should courd, Haveny? Clips, my jod\n",
      "\n",
      "\n",
      "Epoch 00007: loss improved from 1.35714 to 1.33009, saving model to model_best.h5\n",
      "Epoch 8/60\n",
      "438871/438871 [==============================] - 372s 847us/step - loss: 1.3095\n",
      "\n",
      "----- Generating text after Epoch: 7\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" it? Reminds me of us. These white folks\"\n",
      " it? Reminds me of us. These white folks are the part of the thing that I was like, I go to start of the street was the show. I was like, Oh, I dont know what I say that we got to start to t\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" it? Reminds me of us. These white folks\"\n",
      " it? Reminds me of us. These white folks are a show the strange in the moment of them and I want to see a girl of hish best minutes that shite a guy with a criip than youre like, Youre not t\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" it? Reminds me of us. These white folks\"\n",
      " it? Reminds me of us. These white folks here that fucking yinglip up in the night show about this depressive. Helling is what a lot of the room, and I gave them off beef Le Daddaton. And I \n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" it? Reminds me of us. These white folks\"\n",
      " it? Reminds me of us. These white folks and on one. You get down of night with an igrivadit lipper of reople shot everyones, I was all dragg to talk about a talking guys, now I dont like no\n",
      "\n",
      "\n",
      "Epoch 00008: loss improved from 1.33009 to 1.30955, saving model to model_best.h5\n",
      "Epoch 9/60\n",
      "438871/438871 [==============================] - 371s 844us/step - loss: 1.2902\n",
      "\n",
      "----- Generating text after Epoch: 8\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"er ass, uh, you know, grabs her boobs, a\"\n",
      "er ass, uh, you know, grabs her boobs, and then I got the stuff that I got the show and then the back of the back of the way they start to be like, You know? I dont know what the fuck is the\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"er ass, uh, you know, grabs her boobs, a\"\n",
      "er ass, uh, you know, grabs her boobs, and then I go to the opposite of his and they just said to me in the rest of the president that it is, but like, they didnt think thats the baby. I go,\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"er ass, uh, you know, grabs her boobs, a\"\n",
      "er ass, uh, you know, grabs her boobs, all of you? I dont want to find out girl. How gobha? Its a white other courh. Not levs, and everyone, it Your moms of honest wasnt phone restreate in, \n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"er ass, uh, you know, grabs her boobs, a\"\n",
      "er ass, uh, you know, grabs her boobs, a lot of the fact comblect, hand about creation? Stof excifmed, orsed about a clessle. A festrecavious! How dont pee, at :ound. And I tell up modems an\n",
      "\n",
      "\n",
      "Epoch 00009: loss improved from 1.30955 to 1.29016, saving model to model_best.h5\n",
      "Epoch 10/60\n",
      "438871/438871 [==============================] - 393s 896us/step - loss: 1.2739\n",
      "\n",
      "----- Generating text after Epoch: 9\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" saw a second video of a cat giving anot\"\n",
      " saw a second video of a cat giving another shit to the start of the back of the story and then I dont know what I dont know what the fuck? They can see that shit and then I was like, What t\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" saw a second video of a cat giving anot\"\n",
      " saw a second video of a cat giving another sound and shes like, What are you that would have a bank of my day. Its the guy that was a train and I go, Okay. I gotta go on the thing that that\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" saw a second video of a cat giving anot\"\n",
      " saw a second video of a cat giving another grannem was she actually will sup Every woman, isnt it. Right? All right. I give a walk stare up and I colled those phones, on people who dont jus\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" saw a second video of a cat giving anot\"\n",
      " saw a second video of a cat giving another profuckes. theyre kind of names man. MI Youve wry capriqued this. Trainsactions telling a fuck? Every getting six bounch of Rosta want round. That\n",
      "\n",
      "\n",
      "Epoch 00010: loss improved from 1.29016 to 1.27391, saving model to model_best.h5\n",
      "Epoch 11/60\n",
      "438871/438871 [==============================] - 419s 955us/step - loss: 1.2591\n",
      "\n",
      "----- Generating text after Epoch: 10\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"ape juice and shit in like fakeass paper\"\n",
      "ape juice and shit in like fakeass papers and the street and I was like, I dont know what I was like, What are you that they were like, What the fuck are you that the back and I dont know wh\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"ape juice and shit in like fakeass paper\"\n",
      "ape juice and shit in like fakeass papers and the standup came on in the thing that is really probably the woman and dont know if they do that be back to a lot of the stage and the last bett\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"ape juice and shit in like fakeass paper\"\n",
      "ape juice and shit in like fakeass papers. It was in a slive SSD Charge and , you were much. Put! We can stop of the count of making politicalative myself. He goes, Hey, question I can see y\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"ape juice and shit in like fakeass paper\"\n",
      "ape juice and shit in like fakeass paper of drage. Yeah. And thats why one ic confortion loamed. Like, Do you only ever cant be cause puwedato from Thams. Audience aust a. Iw everyone can ju\n",
      "\n",
      "\n",
      "Epoch 00011: loss improved from 1.27391 to 1.25914, saving model to model_best.h5\n",
      "Epoch 12/60\n",
      "438871/438871 [==============================] - 416s 947us/step - loss: 1.2483\n",
      "\n",
      "----- Generating text after Epoch: 11\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \". When did you think the fight might be \"\n",
      ". When did you think the fight might be a baby in the back of the time. I dont know what I do that the same start of the back of the way the same thing that I was like, Why do you do that th\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \". When did you think the fight might be \"\n",
      ". When did you think the fight might be so much when I do not love to the death but thats the things about some girling to a standup fucking dumb in the face. Ill get the bathroom and I was \n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \". When did you think the fight might be \"\n",
      ". When did you think the fight might be cause you round money, but I dont want any jokes about Geomer. audience laughing And then I played Southend Six were not incomfiniting even saw it. Be\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \". When did you think the fight might be \"\n",
      ". When did you think the fight might be womon. Im wrong people shate, but everybody would found to get here. If we un it serroods ssage, as IgetMies about Pver at the ghrstive a leg and wein\n",
      "\n",
      "\n",
      "Epoch 00012: loss improved from 1.25914 to 1.24835, saving model to model_best.h5\n",
      "Epoch 13/60\n",
      "438871/438871 [==============================] - 391s 890us/step - loss: 1.2375\n",
      "\n",
      "----- Generating text after Epoch: 12\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"Right? Look at Obama. When that guy got \"\n",
      "Right? Look at Obama. When that guy got to the thing I want to know that that we can see the state that was a big down to the show. I dont know what I was like, What is there a bit of the st\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"Right? Look at Obama. When that guy got \"\n",
      "Right? Look at Obama. When that guy got to the sound to me. Theyre white my conception with me? I dont think it was real cliter. LAUGHTER I love them the last thing that I had to have to hav\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"Right? Look at Obama. When that guy got \"\n",
      "Right? Look at Obama. When that guy got herself Cause again. You red it. And that high, so if youre funny. Further way to, like, do I take them. So I give him back into a derictreat in runne\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"Right? Look at Obama. When that guy got \"\n",
      "Right? Look at Obama. When that guy got crizit sliving I dont wanna go vide everyday? Not middle. Its one. closasing up down the skin, met pictures of a nervouse bloke? But it seles immediat\n",
      "\n",
      "\n",
      "Epoch 00013: loss improved from 1.24835 to 1.23751, saving model to model_best.h5\n",
      "Epoch 14/60\n",
      "438871/438871 [==============================] - 413s 941us/step - loss: 1.2260\n",
      "\n",
      "----- Generating text after Epoch: 13\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"you noticed that? LAUGHTER Yeah, all the\"\n",
      "you noticed that? LAUGHTER Yeah, all the world was a little bit of the song. I was like, What do you want to go to the party of the last thing and I was like, You know what I was like, Yeah,\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"you noticed that? LAUGHTER Yeah, all the\"\n",
      "you noticed that? LAUGHTER Yeah, all the way dont care the thing words of getting a bit of a curtain control of my connipst, he said, What? I was like, Yes, away, and I am rely to be honest \n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"you noticed that? LAUGHTER Yeah, all the\"\n",
      "you noticed that? LAUGHTER Yeah, all the thing. Im not, the quid to hold the music and youre anticeses you off, you dont know everyone part of Scoof. Why do you gotta be like Yeah. And the s\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"you noticed that? LAUGHTER Yeah, all the\"\n",
      "you noticed that? LAUGHTER Yeah, all the musd for you. so it was funny, how do shit late like Howard Checks Vool. Right? Theyd get. Down through the party clewp cuztametens door down. He was\n",
      "\n",
      "\n",
      "Epoch 00014: loss improved from 1.23751 to 1.22599, saving model to model_best.h5\n",
      "Epoch 15/60\n",
      "438871/438871 [==============================] - 412s 939us/step - loss: 1.2184\n",
      "\n",
      "----- Generating text after Epoch: 14\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"he turns the page and, Ew, ew He fucking\"\n",
      "he turns the page and, Ew, ew He fucking like the statement with a point the thing that we want to see the thing that I got a lot of the way the people who happened to me. I was like, Oh, ye\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"he turns the page and, Ew, ew He fucking\"\n",
      "he turns the page and, Ew, ew He fucking do the metic and the bag of course his park in my chaines that they dont go to the house I dont know the pussy. The more like, I was like, Thats not \n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"he turns the page and, Ew, ew He fucking\"\n",
      "he turns the page and, Ew, ew He fucking standup just dont find me and been created percent of a start, just checked in this cips to show. I dont know what Im an extra month of the kidding k\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"he turns the page and, Ew, ew He fucking\"\n",
      "he turns the page and, Ew, ew He fucking Im trying to fight his aircland eplited. Picks him up from? HeGring Marivia  times, I think this, leave me before. Yeah, I even getting see fucked up\n",
      "\n",
      "\n",
      "Epoch 00015: loss improved from 1.22599 to 1.21840, saving model to model_best.h5\n",
      "Epoch 16/60\n",
      "438871/438871 [==============================] - 414s 943us/step - loss: 1.2078\n",
      "\n",
      "----- Generating text after Epoch: 15\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"nt if youre not a fucking architect, rig\"\n",
      "nt if youre not a fucking architect, right? And I was like, I dont know what I was like, What about this for the back of the world. And I was like, What are you thinking about the world. And\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"nt if youre not a fucking architect, rig\"\n",
      "nt if youre not a fucking architect, right? And I was like, What is that when I was like, Why does they, and I said, Im a good movies so I do that people who was the refer than child, but I \n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"nt if youre not a fucking architect, rig\"\n",
      "nt if youre not a fucking architect, right, Ill do it! Let me party. This is my fair with rext of wind pussy. Um Id be writing hy money. It was just that Ive got no standup with a hotely, bu\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"nt if youre not a fucking architect, rig\"\n",
      "nt if youre not a fucking architect, right, back shit. Smarbettimes, Gonna get to. So, everyone was out. I used to buy her with guys! If you go! Okay? No, I know, Bhesson? She finishes who d\n",
      "\n",
      "\n",
      "Epoch 00016: loss improved from 1.21840 to 1.20784, saving model to model_best.h5\n",
      "Epoch 17/60\n",
      "438871/438871 [==============================] - 413s 942us/step - loss: 1.2040\n",
      "\n",
      "----- Generating text after Epoch: 16\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"ell. When I was , I was in Amsterdam wit\"\n",
      "ell. When I was , I was in Amsterdam with the country that you can see the first time I was like, You know what I did. Its a straight and then I was like, What is the show and the complicati\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"ell. When I was , I was in Amsterdam wit\"\n",
      "ell. When I was , I was in Amsterdam with the window of some reason to show your care and the best good Thatcher. Theyll take the whole thing. And then I could ask you to me and he said, Wel\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"ell. When I was , I was in Amsterdam wit\"\n",
      "ell. When I was , I was in Amsterdam with you. You drive her, and I could never hear glave sexual. And her swaces six yually the extraming, Goddy because then we learned in Atechilas. You ca\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"ell. When I was , I was in Amsterdam wit\"\n",
      "ell. When I was , I was in Amsterdam without politics. An wedderciad, these or Islamian in Loirsacnies. Heres, Do you had been daving a year! I actually dont know mistakes And Im fucking mat\n",
      "\n",
      "\n",
      "Epoch 00017: loss improved from 1.20784 to 1.20403, saving model to model_best.h5\n",
      "Epoch 18/60\n",
      "438871/438871 [==============================] - 410s 935us/step - loss: 1.1965\n",
      "\n",
      "----- Generating text after Epoch: 17\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \". Shes stressed. She wanna say Why you n\"\n",
      ". Shes stressed. She wanna say Why you need to see you to the bedroom the way they want to do it. I was like, I dont know what I was there the pussy on the state that I was like, What is tha\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \". Shes stressed. She wanna say Why you n\"\n",
      ". Shes stressed. She wanna say Why you need to watch the kids to part of the back. I think he was the ten of the school with the state. Im just sitting the box. You know, he said, I was like\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \". Shes stressed. She wanna say Why you n\"\n",
      ". Shes stressed. She wanna say Why you need to know that you do that? Whatever you have it a strip! Its always my challing view, so so thats when Im wural. What about her good? thats the las\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \". Shes stressed. She wanna say Why you n\"\n",
      ". Shes stressed. She wanna say Why you need to follow Americans for you cause thats fucking like very facual. Theres inupre, this most years old! And at my girls indigite bocks in superfecte\n",
      "\n",
      "\n",
      "Epoch 00018: loss improved from 1.20403 to 1.19649, saving model to model_best.h5\n",
      "Epoch 19/60\n",
      "438871/438871 [==============================] - 413s 941us/step - loss: 1.1903\n",
      "\n",
      "----- Generating text after Epoch: 18\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \". audience laughs Enough about me. What \"\n",
      ". audience laughs Enough about me. What is that that I was like, Oh, I dont know what I mean? I dont know what I was like, Oh, theres a street that I was like, What do you want to be like, W\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \". audience laughs Enough about me. What \"\n",
      ". audience laughs Enough about me. What happened to your couch. And I said, I would be like, What are you still doing the guy when you want to be like, Oh, quickly the parasite in the day to\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \". audience laughs Enough about me. What \"\n",
      ". audience laughs Enough about me. What one of a real at the  Ives fun. Kad, like, eognize if Im gonna do? And I knock up, he doesnt iway suffering the iniscallact stuff. All Are overy his s\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \". audience laughs Enough about me. What \"\n",
      ". audience laughs Enough about me. What knest TL time being is like, Tum language ettoms. The arson! You know, theres not rekayositize that? Again? What? Heardoffaholic, its another when the\n",
      "\n",
      "\n",
      "Epoch 00019: loss improved from 1.19649 to 1.19028, saving model to model_best.h5\n",
      "Epoch 20/60\n",
      "438871/438871 [==============================] - 411s 936us/step - loss: 1.1837\n",
      "\n",
      "----- Generating text after Epoch: 19\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"servational comedy. Ive had some good re\"\n",
      "servational comedy. Ive had some good reason that we are a bit of the back of the back of the back of the couch is a couple of the best thing you stop in the back of the party and I said, I \n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"servational comedy. Ive had some good re\"\n",
      "servational comedy. Ive had some good register. They in the same time I said I dont know what the fuck happened to me. I dont know what I want to have to do a crazy best game in the back of \n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"servational comedy. Ive had some good re\"\n",
      "servational comedy. Ive had some good repalled. Him. Maybe effient is one of the kind. You know, you know, like its seconds for you, mate. Ive been applauding Learn have a guy. And then the \n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"servational comedy. Ive had some good re\"\n",
      "servational comedy. Ive had some good rewair at one of now. Lets say as Fun. Fucking way, met and I say, Dont learn up on something why goes, Whos. This kids are amazi favelage in it. Why? I\n",
      "\n",
      "\n",
      "Epoch 00020: loss improved from 1.19028 to 1.18374, saving model to model_best.h5\n",
      "Epoch 21/60\n",
      "438871/438871 [==============================] - 413s 941us/step - loss: 1.1800\n",
      "\n",
      "----- Generating text after Epoch: 20\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"for you? Sex at least, like, if you have\"\n",
      "for you? Sex at least, like, if you have to get it. I got to be like, What are you gonna have to get to the comedian, and then I was like, Oh, they got a fucking person. I was like, What are\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"for you? Sex at least, like, if you have\"\n",
      "for you? Sex at least, like, if you have any side of Nourdy Colorion. Its just like, I have a couple of delivery and then I dont know if you should see a pressure of the property. They can g\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"for you? Sex at least, like, if you have\"\n",
      "for you? Sex at least, like, if you have any dick a share. Im okay. When I said what were in Lientain dancial. And I came here perhembing. LAUGHTER So See who do is LAUGHTER And well they st\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"for you? Sex at least, like, if you have\"\n",
      "for you? Sex at least, like, if you have any Netflix. Im like, Wrint. One profocking, my year at a bit of performs. Its brapilied. Its fucking fair fable, but I guess. You got to see oh. Too\n",
      "\n",
      "\n",
      "Epoch 00021: loss improved from 1.18374 to 1.18002, saving model to model_best.h5\n",
      "Epoch 22/60\n",
      "438871/438871 [==============================] - 410s 933us/step - loss: 1.1765\n",
      "\n",
      "----- Generating text after Epoch: 21\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" moving. My friends are into running. An\"\n",
      " moving. My friends are into running. And then I was like, I dont know what the fuck are you a little bit of things about the show and I was like, Oh, yeah. I dont know what the fuck did you\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" moving. My friends are into running. An\"\n",
      " moving. My friends are into running. And he said, Stand it! He goes, Yeah, I think you have to get in the same of the trans of the theater with the time, but thats what it is. I fucking lau\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" moving. My friends are into running. An\"\n",
      " moving. My friends are into running. And they just tapened these back by school, cause I know I part this somefore gonna forget, Curturic Pigo Meets that the same, like, like, if you dont w\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" moving. My friends are into running. An\"\n",
      " moving. My friends are into running. And my okay indents, way I do be, like, the strip. Yeah, he might be sureback. Kinn and when you say BBC Crimatis Jesus loves that hes looks. Hall with \n",
      "\n",
      "\n",
      "Epoch 00022: loss improved from 1.18002 to 1.17650, saving model to model_best.h5\n",
      "Epoch 23/60\n",
      "438871/438871 [==============================] - 418s 953us/step - loss: 1.1683\n",
      "\n",
      "----- Generating text after Epoch: 22\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \", the whole conversation, he just He jus\"\n",
      ", the whole conversation, he just He just said, I dont know if I was like, What a lot of me and then I was like, Oh, yeah, I was like, I dont know what I mean? I was like, Whats the same thi\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \", the whole conversation, he just He jus\"\n",
      ", the whole conversation, he just He just said, I was like, Do you think about his time. I dont know if theres a lot, they didnt give the wedding strange with the comedians, they would like \n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \", the whole conversation, he just He jus\"\n",
      ", the whole conversation, he just He just told your kids who dont you lost. And then goes, like rentlic versious cuper and both of the New York? But its too crazy is good from and enjoy you \n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \", the whole conversation, he just He jus\"\n",
      ", the whole conversation, he just He just like me. Uh, I had teuch. Where the low mitdats keep with penerts? After fit this bridge apply citheat as we are to hate our best local ryal call yo\n",
      "\n",
      "\n",
      "Epoch 00023: loss improved from 1.17650 to 1.16833, saving model to model_best.h5\n",
      "Epoch 24/60\n",
      "438871/438871 [==============================] - 414s 944us/step - loss: 1.1668\n",
      "\n",
      "----- Generating text after Epoch: 23\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"amme encourages. Small steps. Dont have \"\n",
      "amme encourages. Small steps. Dont have a little bit of the story of the way that was the only thing I was like, I dont know what that is the same thing that I was like, I dont know what I m\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"amme encourages. Small steps. Dont have \"\n",
      "amme encourages. Small steps. Dont have to say. Theres no only life. I love the stay that the comedian as happens and then I was like, Whats what you dont like the window that the only hear \n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"amme encourages. Small steps. Dont have \"\n",
      "amme encourages. Small steps. Dont have to do that with our late for excited, you know? Its more rostup. Like bills dont use Im just trying to pee! But what isst fucking ready for staring he\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"amme encourages. Small steps. Dont have \"\n",
      "amme encourages. Small steps. Dont have done I gotta ext extrains at the one plates? Its bitched in a drinker? I thinks what? No months open. LAUGHTER Its like, Like, I was going on we sound\n",
      "\n",
      "\n",
      "Epoch 00024: loss improved from 1.16833 to 1.16681, saving model to model_best.h5\n",
      "Epoch 25/60\n",
      "438871/438871 [==============================] - 414s 944us/step - loss: 1.1632\n",
      "\n",
      "----- Generating text after Epoch: 24\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"ut being called a dog. They go, Would yo\"\n",
      "ut being called a dog. They go, Would you do that. I dont know what I mean? I dont know what I mean? I dont know what I mean? I dont know what I mean? I dont know what I was like, What are y\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"ut being called a dog. They go, Would yo\"\n",
      "ut being called a dog. They go, Would you know what I want to do the streets. I dont know what that is the comedy pool, its not that that we were a woman, I dont know if you dont know how mu\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"ut being called a dog. They go, Would yo\"\n",
      "ut being called a dog. They go, Would you think its the ones like, That goes like a picture, mate your dick I like to woil the bedetie. Jait him and I realized theyr brides, Stuke, so you do\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"ut being called a dog. They go, Would yo\"\n",
      "ut being called a dog. They go, Would you have kicks, uh, up constryt the poocilstic. You jist poer cared dudeh? They would guy in. Oh, will they learned, Lady lady sitting through me, that \n",
      "\n",
      "\n",
      "Epoch 00025: loss improved from 1.16681 to 1.16325, saving model to model_best.h5\n",
      "Epoch 26/60\n",
      "438871/438871 [==============================] - 423s 963us/step - loss: 1.1578\n",
      "\n",
      "----- Generating text after Epoch: 25\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"kster has logged into my laptop and fuck\"\n",
      "kster has logged into my laptop and fucking like the week. I was like, What about you want to have the guy who do it a movie and then he was like, I was like, What the fuck? They were like, \n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"kster has logged into my laptop and fuck\"\n",
      "kster has logged into my laptop and fucking for a bear guy. You know, thats what they want to have a penis of the DVD and I was always like, This is the fucking sad the world and I was like,\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"kster has logged into my laptop and fuck\"\n",
      "kster has logged into my laptop and fuck it over their crimation, and just in your new days. You guys gets very leaped supper was going to talk to being just Yeah, and the fucking son, youre\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"kster has logged into my laptop and fuck\"\n",
      "kster has logged into my laptop and fucking pirate? Im a life, Or watching our word cwied and she hes a very keepchoes about one. Just walced at the beer with the men mon With . So the car. \n",
      "\n",
      "\n",
      "Epoch 00026: loss improved from 1.16325 to 1.15777, saving model to model_best.h5\n",
      "Epoch 27/60\n",
      "438871/438871 [==============================] - 411s 936us/step - loss: 1.1599\n",
      "\n",
      "----- Generating text after Epoch: 26\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"ng, yeah! Too good to fucking stand up a\"\n",
      "ng, yeah! Too good to fucking stand up and then I was like, What are you the party and I was like, Oh, I dont know what I was the same thing that we would be like, What are you the most be a\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"ng, yeah! Too good to fucking stand up a\"\n",
      "ng, yeah! Too good to fucking stand up and then guys can do a family that its like that anything that could have the last year with the couch and then I walked in the room, because I dont th\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"ng, yeah! Too good to fucking stand up a\"\n",
      "ng, yeah! Too good to fucking stand up ago, which is the jungle part of For, unfortunately would be doing dhat hust him to revent through any of it. Thats what it is, keep of yourself now, b\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"ng, yeah! Too good to fucking stand up a\"\n",
      "ng, yeah! Too good to fucking stand up and Im like, Hell. And demrects who elevenful musie. So, what the fuck! Maybe Im cause Id a bitcoad beam. Id go, You chanded that three house, out of y\n",
      "\n",
      "\n",
      "Epoch 00027: loss did not improve from 1.15777\n",
      "Epoch 28/60\n",
      "438871/438871 [==============================] - 447s 1ms/step - loss: 1.1495\n",
      "\n",
      "----- Generating text after Epoch: 27\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"is that gonna be all? Uh, yeah, no, mayb\"\n",
      "is that gonna be all? Uh, yeah, no, maybe I was like, What the fuck did that say that I was like, I dont know. I was like, I dont know. I dont know. I dont know. I was like, I dont know. I w\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"is that gonna be all? Uh, yeah, no, mayb\"\n",
      "is that gonna be all? Uh, yeah, no, maybe I was like, Oh, yeah, they were all the time. And then he said, Well, thats what musium and the other doctor who could be like, What had the person \n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"is that gonna be all? Uh, yeah, no, mayb\"\n",
      "is that gonna be all? Uh, yeah, no, maybe youre going I go, However, Im gonna do a sij. After myself know the time And then fucking Sextern. But when I go to the looks about any My Popureroo\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"is that gonna be all? Uh, yeah, no, mayb\"\n",
      "is that gonna be all? Uh, yeah, no, maybe Its likelticlly, dont got except me. I can Jare. Like, what is your bidd live behapt. Yeah, theyre gonna be incontrol tonight. Cittog, But looking a\n",
      "\n",
      "\n",
      "Epoch 00028: loss improved from 1.15777 to 1.14953, saving model to model_best.h5\n",
      "Epoch 29/60\n",
      "438871/438871 [==============================] - 430s 981us/step - loss: 1.1483\n",
      "\n",
      "----- Generating text after Epoch: 28\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"Wow, everybody here is a asshole. And th\"\n",
      "Wow, everybody here is a asshole. And then I was like, Oh, I dont know what I mean? I dont know. I dont know what I was like, What the fuck? They do that it. I dont know what I was a strappe\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"Wow, everybody here is a asshole. And th\"\n",
      "Wow, everybody here is a asshole. And the people who stop here. I was like, Oh, I dont think you dont know what that was the people a lot of people that was the way me about the statement to\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"Wow, everybody here is a asshole. And th\"\n",
      "Wow, everybody here is a asshole. And then, I know white was happening. We would say, But they did that binar of night. Im overing the less voting my problem over and I see you at all. The g\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"Wow, everybody here is a asshole. And th\"\n",
      "Wow, everybody here is a asshole. And those quict, but I do as extre to those things to pet these worst on your guy. You look down. No. Have you been, killed you thyse bateray. Loaisn Such a\n",
      "\n",
      "\n",
      "Epoch 00029: loss improved from 1.14953 to 1.14829, saving model to model_best.h5\n",
      "Epoch 30/60\n",
      "438871/438871 [==============================] - 414s 943us/step - loss: 1.1469\n",
      "\n",
      "----- Generating text after Epoch: 29\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" thats what happened. Thats what Yelp di\"\n",
      " thats what happened. Thats what Yelp did you get to the market and I dont know what the fuck did you do it. I dont know what they do it. I was like, What are you thinking about the comedian\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" thats what happened. Thats what Yelp di\"\n",
      " thats what happened. Thats what Yelp did you go to the house? I was like, I dont know what you get against your conception that they were by about the food of a new whole and the marech you\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" thats what happened. Thats what Yelp di\"\n",
      " thats what happened. Thats what Yelp did you making Surveh, bro. I dont have to get flue aridian, which isnt men, and I was like, I dont know if I realized now, its going off droving qual F\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" thats what happened. Thats what Yelp di\"\n",
      " thats what happened. Thats what Yelp did. I love this. Eat my treing.  Chessing, people who had to know. Whats she doesnt applause isnt we before. Which, I am now, I love their coach sourch\n",
      "\n",
      "\n",
      "Epoch 00030: loss improved from 1.14829 to 1.14688, saving model to model_best.h5\n",
      "Epoch 31/60\n",
      "205312/438871 [=============>................] - ETA: 3:39 - loss: 1.1330"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-dcf542d3cc8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    100\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m           callbacks=[print_callback, checkpoint])\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3740\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3742\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \"\"\"\n\u001b[0;32m-> 1081\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1121\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# Cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "# Turn 40-character chunks into vectors for the x. y is the next character in the original sequence\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "\n",
    "# Build the model: 3 layers deep LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, return_sequences=True, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "optimizer = Adam(lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # Helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(150):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print('\\n')\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "# Checkpoint save on new best\n",
    "filepath = \"model_best.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=60,\n",
    "          callbacks=[print_callback, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 40, 256)           324608    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 40, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 40, 256)           525312    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 40, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 60)                15420     \n",
      "=================================================================\n",
      "Total params: 1,390,652\n",
      "Trainable params: 1,390,652\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "438871/438871 [==============================] - 540s 1ms/step - loss: 1.0947\n",
      "\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"into any kind of subculture. But when yo\"\n",
      "into any kind of subculture. But when you were the guy with the way they were pretty streaters and the weekend is a compliment of the window? I was like, I dont know what I mean? I dont know\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"into any kind of subculture. But when yo\"\n",
      "into any kind of subculture. But when you were doing that? I go, Thats like the long to the streets that happened. Like, I gotta be like, Whoa, standup shows and I was like, I just dont know\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"into any kind of subculture. But when yo\"\n",
      "into any kind of subculture. But when you were so pretty deficit shit. If get the other guy, so watch the Hmorth and I readhin, its such a Time sname of solia from a cucture to share great s\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"into any kind of subculture. But when yo\"\n",
      "into any kind of subculture. But when youre in Car To go? Youre like, Hey, Dnoging! Thats what happened. You stuck you off. We have to find. Its like, No! Remember the sluf in the shop as a \n",
      "\n",
      "\n",
      "Epoch 00001: loss improved from 1.14091 to 1.09473, saving model to model_best.h5\n",
      "Epoch 2/60\n",
      "438871/438871 [==============================] - 533s 1ms/step - loss: 1.0734\n",
      "\n",
      "----- Generating text after Epoch: 1\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"meaty. Always a Always a little meanlook\"\n",
      "meaty. Always a Always a little meanlooking in the back of the trainers who think you dont know what I mean? I dont know if you want to see the whole single one of them and I was like, I don\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"meaty. Always a Always a little meanlook\"\n",
      "meaty. Always a Always a little meanlook at the time. Thats it and then I see a chaset that I was the classic dad. Theyre like, Well, thats a lot of people who call them my wife that starts \n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"meaty. Always a Always a little meanlook\"\n",
      "meaty. Always a Always a little meanlooks, and he was like, Oh, I dont end the rule. Its a shop that stepks. But thats not funnally, Im my too, so she comes up. audience laughing"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:55: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " And I would\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"meaty. Always a Always a little meanlook\"\n",
      "meaty. Always a Always a little meanlook and final night, weve all works about I said is when Im in the mirrarity shates, right? There we do. And SNA, Im gonna nine. Its not like hes wearing\n",
      "\n",
      "\n",
      "Epoch 00002: loss improved from 1.09473 to 1.07340, saving model to model_best.h5\n",
      "Epoch 3/60\n",
      "438871/438871 [==============================] - 567s 1ms/step - loss: 1.0625\n",
      "\n",
      "----- Generating text after Epoch: 2\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"es or dreams or supportive friends, fami\"\n",
      "es or dreams or supportive friends, family, I dont know what they did that shit. I was like, This is a little bit different thing. She said, I dont know what I mean? I dont know if you want \n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"es or dreams or supportive friends, fami\"\n",
      "es or dreams or supportive friends, family, the shows the only thing that was a car company. I was like, You know, people are going, Im a two statement. Thats a second half that I have a str\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"es or dreams or supportive friends, fami\"\n",
      "es or dreams or supportive friends, family, you just But what rest of the best I want to hook up in Texan and Friend when it says, Are you doing? So the guy that starts bad. She goes, Whung \n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"es or dreams or supportive friends, fami\"\n",
      "es or dreams or supportive friends, family seem with no, this bit of wowar is . My might be sleeping over, trying it, man. Every second? You couldnt take in leave it school you madbe there w\n",
      "\n",
      "\n",
      "Epoch 00003: loss improved from 1.07340 to 1.06249, saving model to model_best.h5\n",
      "Epoch 4/60\n",
      "438871/438871 [==============================] - 534s 1ms/step - loss: 1.0534\n",
      "\n",
      "----- Generating text after Epoch: 3\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"ly. And he did the thing where he was li\"\n",
      "ly. And he did the thing where he was like, What the fuck? I dont know what I mean? I dont know what I mean? I dont know what I mean? I was like, I dont know if you want to have to do it. I \n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"ly. And he did the thing where he was li\"\n",
      "ly. And he did the thing where he was like, Oh, what the fuck? Im not gonna go to the motherfuckers and thats a man to be in an agree. Im like, Hey, whats that? I go, All right. I dont know \n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"ly. And he did the thing where he was li\"\n",
      "ly. And he did the thing where he was like, Dont worry about Game Gart and Whens what happened. You know, you turn that liberal slibes theyre completely from them and see again, this legs li\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"ly. And he did the thing where he was li\"\n",
      "ly. And he did the thing where he was like, Was in D, annixs. Its hot good, tilk everybody like Theyve impressed him around. Going to the mean when I have from rest. Get recently to get , an\n",
      "\n",
      "\n",
      "Epoch 00004: loss improved from 1.06249 to 1.05336, saving model to model_best.h5\n",
      "Epoch 5/60\n",
      "438871/438871 [==============================] - 536s 1ms/step - loss: 1.0478\n",
      "\n",
      "----- Generating text after Epoch: 4\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"uld like it back. I need that money so I\"\n",
      "uld like it back. I need that money so I was like, Oh, I was like, I dont know what I mean? I dont know what I mean? I dont know what I mean? I dont know what I mean? I dont know what I mean\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"uld like it back. I need that money so I\"\n",
      "uld like it back. I need that money so Im like, Oh, youre fucking disabled the rest of the problem to the garbage to the other day, and I was like, No, thats a wee woman and I do not think I\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"uld like it back. I need that money so I\"\n",
      "uld like it back. I need that money so Im not getting married, and I mean, she just doesnt come back shakes. Now, I dont know how much actually from watching this in the inkescident. And tha\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"uld like it back. I need that money so I\"\n",
      "uld like it back. I need that money so I believe, after God. Im not like, Whoa, what the horm a scanse people look men to side una wet. Its . I was  sitting favorite. Thats it, like, or five\n",
      "\n",
      "\n",
      "Epoch 00005: loss improved from 1.05336 to 1.04775, saving model to model_best.h5\n",
      "Epoch 6/60\n",
      "438871/438871 [==============================] - 502s 1ms/step - loss: 1.0422\n",
      "\n",
      "----- Generating text after Epoch: 5\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" Wednesday afternoon, and I noticed in a\"\n",
      " Wednesday afternoon, and I noticed in a country that I was gonna be like, What are you doing? What do you want to go to the door. Its not a lot of people who have the best thing that was a \n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" Wednesday afternoon, and I noticed in a\"\n",
      " Wednesday afternoon, and I noticed in a movie that you want. I dont know what they did it? I cant stand bondage dog it. I love it. And I was like, Oh, yeah, he was like, All right, I had no\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" Wednesday afternoon, and I noticed in a\"\n",
      " Wednesday afternoon, and I noticed in an asshole confidence. The Physial American were a stranges and now Im a good motherfucker. We well more two Pater, fixst people that notally move in F\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" Wednesday afternoon, and I noticed in a\"\n",
      " Wednesday afternoon, and I noticed in a glam so add Coffee and Normal was gonna come going up of them. Thats the back talt to child obviously. audience laughs Do you have a home. Ill Get th\n",
      "\n",
      "\n",
      "Epoch 00006: loss improved from 1.04775 to 1.04223, saving model to model_best.h5\n",
      "Epoch 7/60\n",
      "438871/438871 [==============================] - 504s 1ms/step - loss: 1.0363\n",
      "\n",
      "----- Generating text after Epoch: 6\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"hard say, Ahahah! Can you please close t\"\n",
      "hard say, Ahahah! Can you please close the whole family. I was like, Oh, I dont wanna go to the guy fucking people and the worst thing I was like, What are you doing? I dont know. I dont kno\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"hard say, Ahahah! Can you please close t\"\n",
      "hard say, Ahahah! Can you please close the guy where you got a real comedians and shit. So I said, Im going to get the time. The weekend that was on the back of the shit in the back of the t\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"hard say, Ahahah! Can you please close t\"\n",
      "hard say, Ahahah! Can you please close there. He trying to sent the streets.YAll did that. So And Picumal has needed to see why They dont of the liaws. And I was called Wital Vesson but of s\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"hard say, Ahahah! Can you please close t\"\n",
      "hard say, Ahahah! Can you please close that. And I think its what guy gets up for foox. Get out with sil atal and each other and it games that ale this show? Ive had fucking Listain, Fas, yo\n",
      "\n",
      "\n",
      "Epoch 00007: loss improved from 1.04223 to 1.03628, saving model to model_best.h5\n",
      "Epoch 8/60\n",
      "438871/438871 [==============================] - 502s 1ms/step - loss: 1.0328\n",
      "\n",
      "----- Generating text after Epoch: 7\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" Oh, Im a male feminist. I totally see t\"\n",
      " Oh, Im a male feminist. I totally see the thing that I was the same being a bit of the statement to a couple of long time. I said, Well, I dont know if you dont know what I mean? I dont kno\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" Oh, Im a male feminist. I totally see t\"\n",
      " Oh, Im a male feminist. I totally see that. And it was a scary minutes, and I was like, Yeah, I dont know if you were trying to do anything about how I dont know if you have to be like, No,\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" Oh, Im a male feminist. I totally see t\"\n",
      " Oh, Im a male feminist. I totally see the whole face. laughter You dont think Im gonna like, youre a waillachious, or your dimpried you by like too much for coming back. I tried is as left \n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" Oh, Im a male feminist. I totally see t\"\n",
      " Oh, Im a male feminist. I totally see that bookly good people. Yeah, this is in orrembassidation of the country. We did it. I see move in that with everything.  Yeah. Oh, Im terrified, wife\n",
      "\n",
      "\n",
      "Epoch 00008: loss improved from 1.03628 to 1.03282, saving model to model_best.h5\n",
      "Epoch 9/60\n",
      "438871/438871 [==============================] - 503s 1ms/step - loss: 1.0287\n",
      "\n",
      "----- Generating text after Epoch: 8\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"rice up. Dyou know, if there were six of\"\n",
      "rice up. Dyou know, if there were six of the point, I was like, What is that? I was like, I dont know what I mean? I was like, I dont know what I mean? I dont know if you were like, What the\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"rice up. Dyou know, if there were six of\"\n",
      "rice up. Dyou know, if there were six of the kids. LAUGHTER And then I was the only thing that not even do you? So, well did that. So I got a meal thing about the start to me. And I was like\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"rice up. Dyou know, if there were six of\"\n",
      "rice up. Dyou know, if there were six of the divorblem and boner. This is right less again out there with his own board. And like, a porno family. The secious americans would look there, I w\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"rice up. Dyou know, if there were six of\"\n",
      "rice up. Dyou know, if there were six of the new New York! Hey, Im, possibly have in final, get in the brare with the sport. And those goas it loated actually bag, Im working out. He did Gen\n",
      "\n",
      "\n",
      "Epoch 00009: loss improved from 1.03282 to 1.02870, saving model to model_best.h5\n",
      "Epoch 10/60\n",
      "438871/438871 [==============================] - 503s 1ms/step - loss: 1.0252\n",
      "\n",
      "----- Generating text after Epoch: 9\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"ever go down. Its just the way up, you k\"\n",
      "ever go down. Its just the way up, you know? I dont know what I mean? I dont know. I was like, What are you doing? You know? I was like, What is this? I was like, I dont know what I mean? I \n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"ever go down. Its just the way up, you k\"\n",
      "ever go down. Its just the way up, you know? But I was like, Oh, yeah, I got the way I said to me and I said, Well, I dont know what you did it? I dont know. Relax. Like a full dick and I wa\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"ever go down. Its just the way up, you k\"\n",
      "ever go down. Its just the way up, you know that? Maybe they could do this one. Right? And then you get announce its getting a swood guy, hoping somebraped. It passed the vein soff. Im getti\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"ever go down. Its just the way up, you k\"\n",
      "ever go down. Its just the way up, you know? Like, the to sit do each.. So, I dont mind she goes, Im not gonna fuck what you blame that dignt you propots time by the thing. Yeah? Look at wom\n",
      "\n",
      "\n",
      "Epoch 00010: loss improved from 1.02870 to 1.02523, saving model to model_best.h5\n",
      "Epoch 11/60\n",
      "438871/438871 [==============================] - 516s 1ms/step - loss: 1.0225\n",
      "\n",
      "----- Generating text after Epoch: 10\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"ound like in the cartoons. I arrive at m\"\n",
      "ound like in the cartoons. I arrive at me and shit. I was like, I dont know what I mean? I dont know what I mean? I dont know. I was like, I dont know what I mean? I dont know what I mean? T\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"ound like in the cartoons. I arrive at m\"\n",
      "ound like in the cartoons. I arrive at me and be like, Yeah, I think what the fuck is the same thing. I was about to get a point of your computer show. I was like, Yeah, I dont know how some\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"ound like in the cartoons. I arrive at m\"\n",
      "ound like in the cartoons. I arrive at my mom as driving for the cunts. I ramely people Auaphy, as your holiday, who did it cant be the second feminist. Im okay, lets go out, and youre not g\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"ound like in the cartoons. I arrive at m\"\n",
      "ound like in the cartoons. I arrive at me about a elite of nhack. LAUGHTER Anyway, I know, mate. Have you being high face. Oh. Why pad do you do? I dont you really Hate my big idea you? She \n",
      "\n",
      "\n",
      "Epoch 00011: loss improved from 1.02523 to 1.02252, saving model to model_best.h5\n",
      "Epoch 12/60\n",
      "438871/438871 [==============================] - 542s 1ms/step - loss: 1.0179\n",
      "\n",
      "----- Generating text after Epoch: 11\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"spectful. Were about to look at the work\"\n",
      "spectful. Were about to look at the work for the stations, and then I was like, What about that? I dont know what I mean? I dont know what I mean? I dont know what I mean? I dont know what I\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"spectful. Were about to look at the work\"\n",
      "spectful. Were about to look at the work. I go, What are you doing? That says I would love to the friends should be two foreplay. And then she was like, Whats happening? I was like, Oh, Ill \n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"spectful. Were about to look at the work\"\n",
      "spectful. Were about to look at the work now, its a political internations about this bit more hologivers? Im just going like, Ill like to get through the ground, thats something I had a bus\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"spectful. Were about to look at the work\"\n",
      "spectful. Were about to look at the work. Thats who I wanted up to him. Lets Kespit, mate. Im sorry the next women. Nobed my army of leans. Hes pretty reading in a goddamned shrooms would co\n",
      "\n",
      "\n",
      "Epoch 00012: loss improved from 1.02252 to 1.01791, saving model to model_best.h5\n",
      "Epoch 13/60\n",
      "438871/438871 [==============================] - 503s 1ms/step - loss: 1.0166\n",
      "\n",
      "----- Generating text after Epoch: 12\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"ina on the planet. Love, Wesley. Oh! I w\"\n",
      "ina on the planet. Love, Wesley. Oh! I was like, What are you doing? What? I dont know what I mean? I dont know. So, I was like, Oh, I was like, I dont know what I mean? I dont know what I m\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"ina on the planet. Love, Wesley. Oh! I w\"\n",
      "ina on the planet. Love, Wesley. Oh! I was like, I dont know what I mean? You know, they can see the guy and no part of the theater, the only time, its not the best fucking animal places. An\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"ina on the planet. Love, Wesley. Oh! I w\"\n",
      "ina on the planet. Love, Wesley. Oh! I want to just learn anything she actually all believes that, so I mean, I didnt mean a sexual phone on the Onday Things they can get hard ever made my m\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"ina on the planet. Love, Wesley. Oh! I w\"\n",
      "ina on the planet. Love, Wesley. Oh! I went back to Gistame tonight is Mock Caesa of the Wsdays, gross. Not in wrock sinker. It is. All right, Each shorts Like, for sure, Im not Mother Over,\n",
      "\n",
      "\n",
      "Epoch 00013: loss improved from 1.01791 to 1.01663, saving model to model_best.h5\n",
      "Epoch 14/60\n",
      " 21504/438871 [>.............................] - ETA: 7:59 - loss: 1.0163"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-beeef69e53a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m           callbacks=[print_callback, checkpoint])\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3740\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3742\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \"\"\"\n\u001b[0;32m-> 1081\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1121\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load the model from h5 file\n",
    "from keras.models import load_model\n",
    "\n",
    "filename = \"model_best.h5\"\n",
    "model = load_model(filename)\n",
    "optimizer = Adam(lr=0.0001) \n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "model.summary()\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=60,\n",
    "          callbacks=[print_callback, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 40, 256)           324608    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 40, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 40, 256)           525312    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 40, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 60)                15420     \n",
      "=================================================================\n",
      "Total params: 1,390,652\n",
      "Trainable params: 1,390,652\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "438871/438871 [==============================] - 533s 1ms/step - loss: 1.0088\n",
      "\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" Im like, They seem happy. Thats one way\"\n",
      " Im like, They seem happy. Thats one way to do that. I was like, I dont know what I mean? I dont know what I mean? I dont know what I mean? I dont know what I mean? I dont know what I mean? \n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" Im like, They seem happy. Thats one way\"\n",
      " Im like, They seem happy. Thats one way to do it. I had to go to the food. And I was like, Oh, I got to go to the party. Theres a completely and my wife said, You should have done anything \n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" Im like, They seem happy. Thats one way\"\n",
      " Im like, They seem happy. Thats one way up I dont disgust to tell a little joy everything anything about : ever been to Concect of Gemming all day. If I saw him and watch Game is I said, So\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" Im like, They seem happy. Thats one way\"\n",
      " Im like, They seem happy. Thats one way to do Like something butter here. LAUGHTER LiTe places he does. : what are you thinking that as? And She did it to be between with a game, so my thro\n",
      "\n",
      "\n",
      "Epoch 00001: loss improved from 1.00956 to 1.00883, saving model to model_best.h5\n",
      "Epoch 2/60\n",
      " 75520/438871 [====>.........................] - ETA: 8:03 - loss: 1.0141"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-3b3e1e984b84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m           callbacks=[print_callback, checkpoint])\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3740\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3742\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \"\"\"\n\u001b[0;32m-> 1081\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1121\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load the model from h5 file\n",
    "from keras.models import load_model\n",
    "\n",
    "filename = \"model_best.h5\"\n",
    "model = load_model(filename)\n",
    "optimizer = Adam(lr=0.00001) \n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "model.summary()\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=60,\n",
    "          callbacks=[print_callback, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the model from h5 file\n",
    "# from keras.models import load_model\n",
    "\n",
    "# filename = \"model_best.h5\"\n",
    "# model = load_model(filename)\n",
    "# optimizer = Adam(lr=0.0001) \n",
    "# model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "# model.summary()\n",
    "\n",
    "# model.fit(x, y,\n",
    "#           batch_size=128,\n",
    "#           epochs=60,\n",
    "#           callbacks=[print_callback, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Translate model input/output with dictionaries\n",
    "# chars = sorted(list(set(text)))\n",
    "# print('total chars:', len(chars))\n",
    "# char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "# indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "# for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "#     print('----- diversity:', diversity)\n",
    "\n",
    "#     generated = ''\n",
    "#     sentence = text[start_index: start_index + maxlen]\n",
    "#     generated += sentence\n",
    "#     print('----- Generating with seed: \"' + sentence + '\"')\n",
    "#     sys.stdout.write(generated)\n",
    "\n",
    "#     for i in range(200):\n",
    "#         x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "#         for t, char in enumerate(sentence):\n",
    "#             x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "#         preds = model.predict(x_pred, verbose=0)[0]\n",
    "#         next_index = sample(preds, diversity)\n",
    "#         next_char = indices_char[next_index]\n",
    "\n",
    "#         sentence = sentence[1:] + next_char\n",
    "\n",
    "#         sys.stdout.write(next_char)\n",
    "#         sys.stdout.flush()\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
