{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Neural Network for Text Generation\n",
    "\n",
    "Here we will take a subset of the full transcript dataset and train a LSTM style Recurrent Network in order to generate new text based on the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import seaborn as sns; sns.set(color_codes=True)\n",
    "import re \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the data from csv file that was created in get_transcripts.ipynb\n",
    "\n",
    "Take only the English transcripts since we will be training our model to produce text in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>tags</th>\n",
       "      <th>date_posted</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>year</th>\n",
       "      <th>transcript</th>\n",
       "      <th>language</th>\n",
       "      <th>runtime</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chris D’Elia: Man on Fire</td>\n",
       "      <td>Chris D'Elia,Stand-up transcripts</td>\n",
       "      <td>April 19th, 2020</td>\n",
       "      <td>https://scrapsfromtheloft.com/2020/04/19/chris...</td>\n",
       "      <td>Chris D'Elia</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>Fire Fire  Man on fire Fire Fire Fire Man on ...</td>\n",
       "      <td>en</td>\n",
       "      <td>65.0</td>\n",
       "      <td>6.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>George Carlin: The Indian Drill Sergeant</td>\n",
       "      <td>George Carlin,Stand-up transcripts</td>\n",
       "      <td>April 13th, 2020</td>\n",
       "      <td>https://scrapsfromtheloft.com/2020/04/13/georg...</td>\n",
       "      <td>George Carlin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In The Indian Sergeant, was emerging as George...</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tom Segura: Ball Hog</td>\n",
       "      <td>Stand-up transcripts,Tom Segura</td>\n",
       "      <td>March 25th, 2020</td>\n",
       "      <td>https://scrapsfromtheloft.com/2020/03/25/tom-s...</td>\n",
       "      <td>Tom Segura</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>How you doing? No shit. Thank you, thank you...</td>\n",
       "      <td>en</td>\n",
       "      <td>70.0</td>\n",
       "      <td>7.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bert Kreischer: Hey Big Boy</td>\n",
       "      <td>Bert Kreischer,Stand-up transcripts</td>\n",
       "      <td>March 21st, 2020</td>\n",
       "      <td>https://scrapsfromtheloft.com/2020/03/21/bert-...</td>\n",
       "      <td>Bert Kreischer</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>Ladies and gentlemen Bert Kreischer! Yeah! Wh...</td>\n",
       "      <td>en</td>\n",
       "      <td>62.0</td>\n",
       "      <td>7.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bert Kreischer: Fighting a Bear</td>\n",
       "      <td>Bert Kreischer,Stand-up transcripts</td>\n",
       "      <td>March 21st, 2020</td>\n",
       "      <td>https://scrapsfromtheloft.com/2020/03/21/bert-...</td>\n",
       "      <td>Bert Kreischer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The machine, Mr. Bert Kreischer, everybody. Le...</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        title  \\\n",
       "0               Chris D’Elia: Man on Fire       \n",
       "1  George Carlin: The Indian Drill Sergeant     \n",
       "2                    Tom Segura: Ball Hog       \n",
       "3             Bert Kreischer: Hey Big Boy       \n",
       "4            Bert Kreischer: Fighting a Bear    \n",
       "\n",
       "                                  tags       date_posted  \\\n",
       "0    Chris D'Elia,Stand-up transcripts  April 19th, 2020   \n",
       "1   George Carlin,Stand-up transcripts  April 13th, 2020   \n",
       "2      Stand-up transcripts,Tom Segura  March 25th, 2020   \n",
       "3  Bert Kreischer,Stand-up transcripts  March 21st, 2020   \n",
       "4  Bert Kreischer,Stand-up transcripts  March 21st, 2020   \n",
       "\n",
       "                                                link            name    year  \\\n",
       "0  https://scrapsfromtheloft.com/2020/04/19/chris...    Chris D'Elia  2017.0   \n",
       "1  https://scrapsfromtheloft.com/2020/04/13/georg...   George Carlin     NaN   \n",
       "2  https://scrapsfromtheloft.com/2020/03/25/tom-s...      Tom Segura  2020.0   \n",
       "3  https://scrapsfromtheloft.com/2020/03/21/bert-...  Bert Kreischer  2020.0   \n",
       "4  https://scrapsfromtheloft.com/2020/03/21/bert-...  Bert Kreischer     NaN   \n",
       "\n",
       "                                          transcript language  runtime  rating  \n",
       "0   Fire Fire  Man on fire Fire Fire Fire Man on ...       en     65.0     6.6  \n",
       "1  In The Indian Sergeant, was emerging as George...       en      NaN     NaN  \n",
       "2    How you doing? No shit. Thank you, thank you...       en     70.0     7.3  \n",
       "3   Ladies and gentlemen Bert Kreischer! Yeah! Wh...       en     62.0     7.1  \n",
       "4  The machine, Mr. Bert Kreischer, everybody. Le...       en      NaN     NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data from csv\n",
    "df = pd.read_csv('StandUpData.csv')\n",
    "df.loc[df.language == 'en']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the transcripts by removing extra whitespace, and unwanted characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With extra whitespace: \n",
      " Fire Fire  Man on fire Fire Fire Fire Man on fire Yes. Yes. All right. Yeah. All right. Just relax.\n",
      "Without extra whitespace:\n",
      "Fire Fire Man on fire Fire Fire Fire Man on fire Yes. Yes. All right. Yeah. All right. Just relax. W\n"
     ]
    }
   ],
   "source": [
    "# Remove leading, trailing, and intermitent whitespace\n",
    "print(\"With extra whitespace: \")\n",
    "print(df.iloc[0].transcript[:100])\n",
    "df.transcript = df.transcript.apply(lambda x: re.sub(\"\\s+\", \" \", x).strip())\n",
    "print(\"Without extra whitespace:\")\n",
    "print(df.iloc[0].transcript[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n",
      "['F', 'i', 'r', 'e', ' ', 'M', 'a', 'n', 'o', 'f', 'Y', 's', '.', 'A', 'l', 'g', 'h', 't', 'J', 'u', 'x', 'W', 'c', 'k', ',', 'd', 'C', 'I', 'm', 'p', 'y', 'b', 'L', 'w', 'U', '?', 'z', 'H', 'E', 'T', ':', 'j', 'v', 'N', 'R', 'S', 'O', 'B', '-', 'P', 'q', 'D', '!', 'V', 'G', 'K', '–', '—', ';', '/', 'Z', \"'\", 'é', 'ú', '%', '*', 'Q', '$', 'X', '#', '″', '£', '&', 'ç', 'í', 'ö', 'ä', 'ô', 'ñ', 'Ç', 'è', '@', 'ü', 'Ü', 'ó', '¿', 'á', 'É', 'Ó', '×', '\\\\', ']', '¡', '_', 'å', 'à', '®', 'ê', 'ù', 'Ã', 'ƒ', 'Â', 'º', '`', '+', 'Á', 'ì', 'È', 'ò', '′', '¶', 'û', '♫', 'Ö', '¢', 'ﬂ', '♬', '»', '=', '\\u200b', 'â', '~', '>']\n"
     ]
    }
   ],
   "source": [
    "# Collect all charcters to see which ones are present in the transcripts\n",
    "chars = []\n",
    "df.transcript.apply(lambda x: [chars.append(char) for char in x if char not in chars])\n",
    "\n",
    "print(len(chars))\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all unwanted characters\n",
    "remove = ['-', '–', '—', '/', 'é', 'ú', '%', '*', '$', '#', '″', '£', '&', 'ç', 'í', 'ö', 'ä', 'ô', 'ñ', 'Ç', 'è', '@', 'ü', 'Ü', 'ó', '¿', 'á', 'É', 'Ó', '×', '\\\\', ']', '¡', '_', 'å', 'à', '®', 'ê', 'ù', 'Ã', 'ƒ', 'Â', 'º', '`', '+', 'Á', 'ì', 'È', 'ò', '′', '¶', 'û', '♫', 'Ö', '¢', 'ﬂ', '♬', '»', '=', '\\u200b', 'â', '~', '>']\n",
    "df.transcript = df.transcript.apply(lambda x: ''.join([char for char in x if char not in remove]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a variable 'text' that will serve as the corpus for our model\n",
    "This simply adds each transcript together into a single string. Instead of using the full corpus, we will only take 1/10th of it to save on processing time and because this is just for fun. A more powerful generator could be made with the resources to process the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13166511"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ''\n",
    "for i in df.transcript:\n",
    "    text += (i + ' ')\n",
    "\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1316651"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shorten text by 1/10 to save processing time\n",
    "portion = int(len(text)/10)\n",
    "text = text[:portion]\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model: LSTM 256 -> LSTM 256 -> LSTM 256 -> Dense\n",
    "Much of this block is taken right from the Keras Docs. I have changed the network architecture, optimizer settings, and callback functions.\n",
    "\n",
    "Learning rate starts out at 0.001 and is allowed to run until it plateaus after about 30 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# Cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "# Turn 40-character chunks into vectors for the x. y is the next character in the original sequence\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "\n",
    "# Build the model: 3 layers deep LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, return_sequences=True, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "optimizer = Adam(lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # Helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(150):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print('\\n')\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "# Checkpoint save on new best\n",
    "filepath = \"model_best.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=60,\n",
    "          callbacks=[print_callback, checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some mid-training gems generated by the model...\n",
    "- Epoch 10: \"I make you have a song for the porn before you think you're a good point, like, I'll give you an example. I was like, no.\"\n",
    "- Epoch 18: \"I make a bank that we're gonna be like, I don't know what the fuck I want. I don't know what the fuck I want to do it. And I don't know what the fuck I want.\"\n",
    "\n",
    "\n",
    "This is good! They sound like English, sort of. Not bad for character-level predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning is resumed with a smaller learning rate\n",
    "Now the Adam optimizer learning rate is set to 0.0001 for another 13 epochs.\n",
    "\n",
    "- Epoch 49: \"Its not what I meant, the thing the car the man didn't want to do it. I don't know what to do. It wasn't any good anyway.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=0.0001) \n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "model.summary()\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=60,\n",
    "          callbacks=[print_callback, checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning is resumed with an even smaller learning rate\n",
    "Finally the learning rate is set to 0.00001 for just 1 more epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=0.00001) \n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "model.summary()\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=60,\n",
    "          callbacks=[print_callback, checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-load the model from 'model_best.h5' if needed, then produce reults\n",
    "A great way to save your work is to use the keras callback ModelCheckpoint which can save the entire model so that you can come back later to continue training. Here I am using the model_best.h5 file to reload the model so that we can use it to generate our text for us. \n",
    "#### I will only be using temperatures 0.5 and 1.0 since those seemed to produce the best results during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 40, 256)           324608    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 40, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 40, 256)           525312    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 40, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 60)                15420     \n",
      "=================================================================\n",
      "Total params: 1,390,652\n",
      "Trainable params: 1,390,652\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load the model from h5 file\n",
    "from keras.models import load_model\n",
    "\n",
    "filename = \"model_best.h5\"\n",
    "model = load_model(filename)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"You know what I mean? My mom used to be \"\n",
      "You know what I mean? My mom used to be like, When when you live at the room, like, Were gonna need to take a burned on here for a compliment for no reason for where I first come from the last thing I was like, What a guy fuck? Thats not the same hands on your crothing sling. You know what I mean? I had a baby. I was like, Oh, do you not be some shit? And then I had done that I love the jokes and I go to the received of the back of a sa\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"You know what I mean? My mom used to be \"\n",
      "You know what I mean? My mom used to be on a market shit about a ruin presidence of expecting by the first time. Just a Bohuwain, Hey, haya. She made you a drag. Its like, Thank you. a picture for expensive, saying, Noing news know, suck, would you go crazy? Shine of the trats that has the classrow lives of Aborting for your viswed into the under it. And Sorry, we did spit. I can really see you anything registry on a movie. I like to go\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Grab some random seed text and predict the next letters\n",
    "start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "for diversity in [0.5, 1.0]:\n",
    "    print('----- diversity:', diversity)\n",
    "\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "\n",
    "    for i in range(400):\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_char = indices_char[next_index]\n",
    "\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "- In the end, the generated text looks like english with only a few spelling errors though it doesn't really make much sense. There are grammar and syntax errors everywhere but this is partially to be expected given that the source text is composed of transcripts from spoken stand-up comedy routines. Unfortuantly, due to computational and time constraints, the full corpus was not used. It is possible that the results of this network with 90% more training data would be significantly different.\n",
    "\n",
    "- There is clearly a lot of work to be done before actually funny text could be created with this LSTM model... Unless you're into absurdist humor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
